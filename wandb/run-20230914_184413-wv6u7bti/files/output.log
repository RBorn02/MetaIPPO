Started worker 0
Started worker 1
Started worker 2
Started worker 3
Started worker 4
Started worker 5
Started worker 6
Started worker 7
Initializing workers...
Parameter containing:
tensor([[0., 0., 0., 0.]], requires_grad=True)
Predicted values mean: -0.0002203698386438191; Returns mean: -5.447873402397718e-11; Advantages mean: -0.0029834071174263954; Values mean: 0.002983406651765108
Actions mean: -0.025908391922712326; Actions std: 0.9941040873527527
Action logprob mean: -5.653250217437744; Action logprob std: 1.3554811477661133
Agent_0 loss total: -0.006454683199990541; pg loss: -0.0010665440579032404; value loss: 0.0002872633322112961; entropy loss: 5.6754021644592285; explained variance: -363484759982079.0; clip fracs: 0.0
Parameter containing:
tensor([[0., 0., 0., 0.]], requires_grad=True)
Predicted values mean: 0.0003030291700270027; Returns mean: -2.4574547952127546e-10; Advantages mean: -0.030268847942352295; Values mean: 0.030268847942352295
Actions mean: -0.02508009970188141; Actions std: 0.9985471963882446
Action logprob mean: -5.670842170715332; Action logprob std: 1.4726217985153198
Agent_1 loss total: -0.005254091738606803; pg loss: -0.0008308419415470336; value loss: 0.0012540656825876795; entropy loss: 5.67731511592865; explained variance: -284122958266367.0; clip fracs: 0.0
Time to update policy: 1.133789300918579
Epoch: 1
0.0 tensor(0.)
Agent_0: Completed 128.0 Episodes;
Total Reward: 0.0; Average Reward This Epoch: 0.0; Rolling Average Reward: 0.0 Best Average Reward: 0.0
Successes: 0.0; Success Rate: 0.0; Rolling Average Sucess Rate: 0.0; Best Rolling Average Sucess Rate: 0.0
Stage 1: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
Stage 2: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
Stage 3: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
0.0 tensor(0.)
Agent_1: Completed 128.0 Episodes;
Total Reward: 0.0; Average Reward This Epoch: 0.0; Rolling Average Reward: 0.0 Best Average Reward: 0.0
Successes: 0.0; Success Rate: 0.0; Rolling Average Sucess Rate: 0.0; Best Rolling Average Sucess Rate: 0.0
Stage 1: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
Stage 2: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
Stage 3: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
Parameter containing:
tensor([[-0.0011,  0.0018, -0.0024,  0.0011]], requires_grad=True)
Predicted values mean: 0.0035388164687901735; Returns mean: 0.003014144953340292; Advantages mean: -0.0038639388512820005; Values mean: 0.0068780831061303616
Actions mean: 0.010104922577738762; Actions std: 1.005307674407959
Action logprob mean: -5.696435928344727; Action logprob std: 1.398890495300293
Agent_0 loss total: -0.005253969997284003; pg loss: -0.0002614046500184486; value loss: 0.0006833816812559235; entropy loss: 5.675946742296219; explained variance: -0.17608261108398438; clip fracs: 0.0
Parameter containing:
tensor([[ 0.0029,  0.0008,  0.0002, -0.0005]], requires_grad=True)
Predicted values mean: 0.0010608118027448654; Returns mean: 0.0030141444876790047; Advantages mean: 0.008343356661498547; Values mean: -0.0053292131051421165
Actions mean: 0.002722303383052349; Actions std: 1.0023932456970215
Action logprob mean: -5.684370517730713; Action logprob std: 1.4148781299591064
Agent_1 loss total: -0.005096353866974823; pg loss: -0.00020267544224310263; value loss: 0.0007863592336434522; entropy loss: 5.680037349462509; explained variance: -0.3323410749435425; clip fracs: 0.0
Time to update policy: 1.0005240440368652
Epoch: 2
0.0 tensor(0.0020)
Agent_0: Completed 128.0 Episodes;
Total Reward: 0.5; Average Reward This Epoch: 0.00390625; Rolling Average Reward: 0.001953125 Best Average Reward: 0.001953125
Successes: 0.0; Success Rate: 0.0; Rolling Average Sucess Rate: 0.0; Best Rolling Average Sucess Rate: 0.0
Stage 1: 1.0 Successes; 128.0 Samples; 0.007812499999389648 Success Rate; Coop Success Rate: 0.007812499999389648; Single Success Rate: 0.0
Stage 2: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
Stage 3: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
0.0 tensor(0.0020)
Agent_1: Completed 128.0 Episodes;
Total Reward: 0.5; Average Reward This Epoch: 0.00390625; Rolling Average Reward: 0.001953125 Best Average Reward: 0.001953125
Successes: 0.0; Success Rate: 0.0; Rolling Average Sucess Rate: 0.0; Best Rolling Average Sucess Rate: 0.0
Stage 1: 1.0 Successes; 128.0 Samples; 0.007812499999389648 Success Rate; Coop Success Rate: 0.007812499999389648; Single Success Rate: 0.0
Stage 2: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
Stage 3: 0.0 Successes; 128.0 Samples; 0.0 Success Rate; Coop Success Rate: 0.0; Single Success Rate: 0.0
Saved models for agents with average reward 0.00390625
Traceback (most recent call last):
  File "/home/rbornema/Documents/GitHub/MetaIPPO/mp_train.py", line 506, in <module>
    if all(np.array(done, dtype=bool)):
  File "<string>", line 2, in __getitem__
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/managers.py", line 817, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
KeyboardInterrupt
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/rbornema/Documents/GitHub/MetaIPPO/mp_train.py", line 405, in <module>
    with mp.Manager() as manager:
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/managers.py", line 656, in __exit__
    self.shutdown()
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/managers.py", line 674, in _finalize_manager
    process.join(timeout=1.0)
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/rbornema/Documents/GitHub/MetaIPPO/mp_train.py", line 506, in <module>
    if all(np.array(done, dtype=bool)):
  File "<string>", line 2, in __getitem__
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/managers.py", line 817, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
KeyboardInterrupt
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/rbornema/Documents/GitHub/MetaIPPO/mp_train.py", line 405, in <module>
    with mp.Manager() as manager:
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/managers.py", line 656, in __exit__
    self.shutdown()
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/managers.py", line 674, in _finalize_manager
    process.join(timeout=1.0)
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/rbornema/anaconda3/envs/testtorch/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt