Started worker 0
Started worker 1
Started worker 2
Started worker 3
Started worker 4
Started worker 5
Started worker 6
Started worker 7
Initializing workers...
Parameter containing:
tensor([[0., 0., 0., 0.]], requires_grad=True)
Predicted values mean: 0.018679814413189888; Returns mean: 0.01693125069141388; Advantages mean: -0.013563115149736404; Values mean: 0.030494369566440582
Actions mean: 0.012750181369483471; Actions std: 0.9975457191467285
Action logprob mean: -5.666155815124512; Action logprob std: 1.4331556558609009
Agent_0 loss total: 0.01811713565257378; pg loss: -0.00032076175957307385; value loss: 0.02411335814394988; entropy loss: 5.675460636615753; explained variance: -0.11484706401824951; clip fracs: 0.0
Parameter containing:
tensor([[0., 0., 0., 0.]], requires_grad=True)
Predicted values mean: 0.004818941466510296; Returns mean: 0.01693125069141388; Advantages mean: 0.0367896668612957; Values mean: -0.01985841989517212
Actions mean: 0.009937073104083538; Actions std: 0.9997671246528625
Action logprob mean: -5.6748948097229; Action logprob std: 1.416448950767517
Agent_1 loss total: 0.020794939264305867; pg loss: -0.00025338536618624286; value loss: 0.026724918207037263; entropy loss: 5.6765934228897095; explained variance: -0.352819561958313; clip fracs: 0.0
Time to update policy: 4.703661203384399
Traceback (most recent call last):
  File "/home/rbornema/Documents/GitHub/MetaIPPO/mp_train.py", line 486, in <module>
    if update % (update_ratio-1) == 0: #and update != 0:
ZeroDivisionError: integer division or modulo by zero